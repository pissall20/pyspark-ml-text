{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import re\n",
    "import math\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import  StringType, DoubleType, IntegerType, ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(c):\n",
    "    c = F.lower(c)\n",
    "    c = F.regexp_replace(c, \"^rt \", \"\")\n",
    "    c = F.regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "    c = F.regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "    #c = split(c, \"\\\\s+\") tokenization...\n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files\n",
    "posTXT = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"True.csv\")\n",
    "negTXT = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"Fake.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some elements in text column are empty, so we will drop them\n",
    "# Clean the text using above utility function\n",
    "\n",
    "posTXT = posTXT.select(\"text\").dropna().withColumn(\"text\", clean_text(F.col(\"text\")))\n",
    "negTXT = negTXT.select(\"text\").dropna().withColumn(\"text\", clean_text(F.col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both dataframes while adding a label column for marking true/fake\n",
    "full_df = posTXT.withColumn(\"label\", F.lit(1)).union(negTXT.withColumn(\"label\", F.lit(0))).dropDuplicates().withColumn(\"document_id\", F.monotonically_increasing_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"vector\")\n",
    "vector_df = tokenizer.transform(full_df)\n",
    "\n",
    "vector_df = vector_df.withColumn(\"vector\", F.expr(\"filter(vector, x -> x != '')\"))\n",
    "# vector_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Define a list of stop words or use default list\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() \n",
    "\n",
    "# Specify input/output columns\n",
    "remover.setInputCol(\"vector\")\n",
    "remover.setOutputCol(\"vector_no_stopw\")\n",
    "\n",
    "# Transform existing dataframe with the StopWordsRemover\n",
    "vector_no_stopw_df = remover.transform(vector_df).drop(\"vector\")\n",
    "\n",
    "# Display\n",
    "# vector_no_stopw_df.printSchema()\n",
    "# vector_no_stopw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashingTF = HashingTF(inputCol=\"vector_no_stopw\", outputCol=\"rawFeatures\", numFeatures=32)\n",
    "# featurizedData = hashingTF.transform(vector_no_stopw_df) \n",
    "# featurizedData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: TF-IDF implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default vocab size is 262144, removing the vocabsize will automatically take maximum size\n",
    "vocab_size = 40 \n",
    "\n",
    "# Calculating term frequency\n",
    "cv = CountVectorizer(inputCol=\"vector_no_stopw\", outputCol=\"rawFeatures\", minTF=2.0, minDF=2.0, vocabSize=vocab_size)\n",
    "cv_model = cv.fit(vector_no_stopw_df)\n",
    "featurizedData = cv_model.transform(vector_no_stopw_df)\n",
    "# featurizedData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating inverse data frequency from term frequency\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2)\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData) \n",
    "# rescaledData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "| vocabList|  counts|\n",
      "+----------+--------+\n",
      "|      said|112723.0|\n",
      "|     trump| 99400.0|\n",
      "|        us| 46437.0|\n",
      "| president| 34159.0|\n",
      "|    people| 26758.0|\n",
      "|       one| 19924.0|\n",
      "|   reuters| 11521.0|\n",
      "|     state| 21064.0|\n",
      "|      also| 16649.0|\n",
      "|       new| 18978.0|\n",
      "|    donald| 12242.0|\n",
      "|    states| 17676.0|\n",
      "|     house| 18282.0|\n",
      "|government| 17579.0|\n",
      "|republican| 17307.0|\n",
      "|    united| 14919.0|\n",
      "|      told| 11427.0|\n",
      "|   clinton| 17686.0|\n",
      "|     obama| 15648.0|\n",
      "|     white| 14028.0|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total counts of words\n",
    "import numpy as np\n",
    "\n",
    "total_counts = rescaledData.select('rawFeatures').rdd\\\n",
    "                    .map(lambda row: row['rawFeatures'].toArray())\\\n",
    "                    .reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "\n",
    "# Vocabulary and it's count\n",
    "vocabList = cv_model.vocabulary\n",
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return F.udf(termsIdx2Term, ArrayType(StringType()))\n",
    "\n",
    "# Helper functions to extract indices and values from SparseMatrix type columns\n",
    "indices_udf = F.udf(lambda vector: vector.indices.tolist(), ArrayType(IntegerType()))\n",
    "values_udf = F.udf(lambda vector: vector.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "# TFIDF is the product of tf, idf\n",
    "tf_idf_udf = F.udf(lambda x_vec, y_vec: [x*y for x, y in zip(x_vec, y_vec)], ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the TF and TF-IDF values\n",
    "\n",
    "tf_idf = rescaledData.withColumn('indices', indices_udf(F.col('rawFeatures')))\\\n",
    "           .withColumn('tf_values', values_udf(F.col('rawFeatures')))\\\n",
    "           .withColumn('idf_values', values_udf(F.col('features')))\\\n",
    "           .withColumn(\"terms\", termsIdx2Term(vocabList)(\"indices\"))\\\n",
    "           .withColumn(\"tfidf_values\", tf_idf_udf(F.col(\"tf_values\"), F.col(\"idf_values\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_df = tf_idf\\\n",
    "    .withColumn(\"word_index\", F.array([F.lit(x) for x in vocabList]))\\\n",
    "    .withColumn(\"tmp\", F.arrays_zip(\"word_index\", \"tf_values\", \"idf_values\", \"tfidf_values\"))\\\n",
    "    .withColumn(\"tmp\", F.explode(\"tmp\"))\\\n",
    "    .select(\"document_id\", \"text\", \"label\", F.col(\"tmp.word_index\"), F.col(\"tmp.tf_values\"), F.col(\"tmp.idf_values\"), F.col(\"tmp.tfidf_values\"))\n",
    "\n",
    "# Print this if you want to see TF, IDF values.\n",
    "\n",
    "# main_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: We do see an interesting connection between rare words and high tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+----------+------------------+\n",
      "|label|  document_id|                text|word_index|      tfidf_values|\n",
      "+-----+-------------+--------------------+----------+------------------+\n",
      "|    0| 438086664346|wow what a list o...|     first|13768.321835306568|\n",
      "|    0| 824633721023|patrick henningse...|   clinton|10505.923791244139|\n",
      "|    0|1640677507272|fundamental trans...|     years|  9614.74156045434|\n",
      "|    0|  17179869362|one of the ancill...|     state| 9223.714743325312|\n",
      "|    0| 360777253053|thanks to obama s...|     state| 8962.062427137105|\n",
      "|    0|1322849927357|shawn helton 21st...|   clinton| 8395.574085702672|\n",
      "|    0| 438086664346|wow what a list o...| president| 7123.014543631126|\n",
      "|    0| 429496729731|it s thursday jul...|     obama| 6716.152034085382|\n",
      "|    0| 867583393971| patrick hennings...|   clinton|  5556.85225322004|\n",
      "|    0| 867583393970| in response to t...|      news| 5499.028624532774|\n",
      "|    0| 678604832916|hillary won t be ...|   clinton| 4669.299462775172|\n",
      "|    0|1073741824188|patrick henningse...|      news| 4368.557239054629|\n",
      "|    0|   8589934780|i just happened t...|     trump| 4292.857705470188|\n",
      "|    0| 755914244294|niraj srivastava ...|     trump|4044.0502086998567|\n",
      "|    0|1408749273273|shawn helton   21...|     trump|3922.4316188606276|\n",
      "|    0|1305670058147|houston texas tri...|   clinton|  3858.92517584725|\n",
      "|    0|1331439861960| robert parry con...|   clinton|  3858.92517584725|\n",
      "|    0|1597727834290|a speech for the ...|    united|3753.6340584742234|\n",
      "|    1|1692217114625|reuters  the us c...|   percent| 3651.506360318965|\n",
      "|    1| 395136991232|reuters  the us h...|   percent| 3651.506360318965|\n",
      "+-----+-------------+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is the final output with sorted tf-idf values\n",
    "\n",
    "main_df.select(\"label\", \"document_id\", \"text\", \"word_index\", \"tfidf_values\").orderBy(\"tfidf_values\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Use-case\n",
    "\n",
    "A social network is interested in producing an engine for filtering fake news. For this purpose they want to run your algorithm on every post that is published. The goal is to verify the correctness of the news in the post as quickly as possible and publish it if it is correct and archived otherwise. Describe the technological difficulties of this type of system, especially regarding the storage of unstructured data and real-time information processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
